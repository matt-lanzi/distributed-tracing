
# ğŸ§  Distributed Log Normalization & Traceability POC

This proof of concept (POC) demonstrates how to ingest logs from distributed systems â€” even when they follow inconsistent formats â€” and normalize them into a unified schema for observability, traceability, and historical analysis.

---

## âœ… Overview

This system:

- Ingests logs via **Kafka** from multiple distributed systems (structured and unstructured)
- Normalizes log events to a consistent schema
- Joins multi-part logs (e.g., events and error details)
- Stores results in **SQLite** for querying and analysis
- Provides test coverage for key modules
- Includes Kafka UI (optional) for inspecting topic messages

---

## ğŸ— Architecture

```
+-------------------+      +------------------+      +------------------+      +-------------------+
|   fedebom         |      |      cmf         |      |      bcis        |      |    reporting      |
+-------------------+      +------------------+      +------------------+      +-------------------+
        |                          |                        |                          |
        +-----------+--------------+------------------------+--------------------------+
                                    |
                                    v
                         +----------------------+
                         |   Kafka Topic:       |
                         |      raw-logs        |
                         +----------------------+
                                    |
                                    v
                         +----------------------+
                         |   Kafka Consumer     |
                         |   (Normalizer)       |
                         +----------------------+
                                    |
                                    v
                         +----------------------+
                         |   SQLite Database    |
                         |   event_history.db   |
                         +----------------------+
                                    |
                                    v
                         +----------------------+
                         | Export / Analytics   |
                         +----------------------+
```


---

## ğŸ’¾ Normalized Schema

| Field           | Description                                |
|----------------|--------------------------------------------|
| `system_id`     | Originating system (e.g. `fedebom`, `bcis`) |
| `checkpoint_id` | Processing stage or label                  |
| `timestamp`     | Event timestamp (ISO format)               |
| `status`        | `SUCCESS` or `FAILURE`                     |
| `correlation_id`| Trace ID for joining related events        |
| `failure_reason`| Optional reason for failure (if any)       |

---

## ğŸ”Œ Systems Overview

Each system (producer) represents a different real-world logging style, allowing us to demonstrate normalization of diverse formats.

| System       | Format             | Failure Detail             | Notes                                                                 |
|--------------|--------------------|-----------------------------|-----------------------------------------------------------------------|
| `fedebom`    | Structured JSON     | Separate JSON log           | Emits a checkpoint log, and if failed, emits a second log with error detail |
| `reporting`  | Structured JSON     | Inline in log               | Emits one structured log that includes both the event and error fields |
| `bcis`       | Unstructured text   | Inline in log               | Emits plain-text logs with both status and failure reason included    |
| `cmf`        | Unstructured text   | Separate plain-text log     | Similar to `fedebom` but logs are in plain text instead of JSON       |

---

## ğŸ—‚ Supported Systems and Log Examples

### âœ… `fedebom` (structured, multi-log join)
- **Main log:** JSON with `status` and `checkpoint`
- **Failure reason:** separate log with same `correlation_id`

**Checkpoint log:**
```
{
  "system": "fedebom",
  "checkpoint": "CHECKPOINT_1",
  "status": "FAILURE",
  "timestamp": "2025-05-04T10:30:00",
  "correlation_id": "abc-123"
}
```

**Separate error log:**
```
{
  "system": "fedebom",
  "correlation_id": "abc-123",
  "failure_reason": "FAILURE_REASON_2"
}
```
### âœ… `reporting` (structured, inline error)
- Single JSON log with failure reason inline if `status == "FAILURE"`
```
{
  "system": "reporting",
  "checkpoint": "REPORTING_CHECKPOINT_2",
  "status": "FAILURE",
  "timestamp": "2025-05-04T10:33:00",
  "correlation_id": "ghi-789",
  "failure_reason": "REPORTING_FAILURE_REASON_2"
}
```


### âœ… `cmf` (unstructured, multi-log join)
**Checkpoint log:**
```
2025-05-04T10:32:00 CMF: CMF_CHECKPOINT_2 - FAILURE (ID: def-456)
```

**Separate error log:**
```
2025-05-04T10:32:01 CMF CMF_ERROR: (ID: def-456) - CMF_FAILURE_REASON_1
```

### âœ… `bcis` (unstructured, single-log)
**Checkpoint log:**
```
2025-05-04T10:30:00 BCIS: CHECKPOINT_B - FAILURE (ID: abc-123) - FAILURE_REASON_X
```
```
2025-05-04T10:30:00 BCIS: CHECKPOINT_A - SUCCESS (ID: abc-123)
```
---

Example Flow from Orchestrator:
-------------------------------
```
1. Orchestrator starts and triggers a new correlation_id

2. Fedebom system begins:
   â†’ INITIATED (SUCCESS)
   â†’ CHECKPOINT_1 (FAILURE)
     â†³ [Failure reason emitted separately]
   â†’ CHECKPOINT_1 (SUCCESS)
   â†’ CHECKPOINT_2 (SUCCESS)
   â†’ FINALIZED (SUCCESS)

3. Orchestrator randomly chooses the next downstream path:
   a. Fedebom â†’ CMF â†’ Reporting and BCIS in parallel
   b. Fedebom â†’ Reporting and BCIS in parallel

4. If CMF is chosen:
   â†’ CMF: CMF_CHECKPOINT_1 (SUCCESS)
   â†’ CMF: CMF_CHECKPOINT_2 (FAILURE)
     â†³ CMF_ERROR log sent separately with failure reason
   â†’ CMF: CMF_CHECKPOINT_2 (SUCCESS)
   â†’ CMF: CMF_FINALIZED (SUCCESS)

5. Reporting system logs:
   â†’ REPORTING_INITIATED â†’ REPORTING_CHECKPOINT_1 â†’ ... â†’ REPORTING_FINALIZED  


6. BCIS system logs:
   â†’ BCIS_INITIALIZED â†’ BCIS_CHECKPOINT_1 â†’ ... â†’ BCIS_FINALIZED  


7. Each step emits logs to Kafka â†’ parsed â†’ normalized â†’ stored in SQLite
```

---

## ğŸ“‚ Project Structure

```bash
distributed-tracing/
â”œâ”€â”€ README.md
â”œâ”€â”€ orchestrator.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ schema_setup.py
â”‚   â””â”€â”€ event_history.db
â”‚
â”œâ”€â”€ consumer.py
â”‚
â”œâ”€â”€ flow_engine/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ flow_engine.py
â”‚
â”œâ”€â”€ log_parsers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ structured.py
â”‚   â”œâ”€â”€ bcis.py
â”‚   â””â”€â”€ cmf.py
â”‚
â”œâ”€â”€ producers/
â”‚   â”œâ”€â”€ producer_fedebom.py
â”‚   â”œâ”€â”€ producer_cmf.py
â”‚   â”œâ”€â”€ producer_bcis.py
â”‚   â””â”€â”€ producer_reporting.py
â”‚
â”œâ”€â”€ kui/
â”‚   â””â”€â”€ config.yml
â”‚
# â””â”€â”€ tests/
#     â”œâ”€â”€ conftest.py
#     â”œâ”€â”€ test_flow_engine.py
#     â””â”€â”€ test_parsers/
#         â”œâ”€â”€ test_structured.py
#         â”œâ”€â”€ test_bcis.py
#         â””â”€â”€ test_cmf.py
```

---

## ğŸš€ Getting Started

### 1. Create Virtual Environment & Install Dependencies

```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Start Kafka + Zookeeper + Kafka UI

```bash
docker-compose up -d
```

### 3. Set Up SQLite Schema

```bash
python db/schema_setup.py
```

### 4. Run the Consumer

```bash
python consumer.py
```

### 5. Run the Orchestrator

```bash
python orchestrator.py
```

---

## ğŸ§ª Run Tests

```bash
PYTHONPATH=. pytest tests/
```

---



---

## ğŸ§° Helpful Commands

### View Kafka Logs Persisted
```bash
sqlite3 db/event_history.db
SELECT * FROM events;
```

### Export to Pandas (in Python)
```python
import pandas as pd
df = pd.read_sql("SELECT * FROM events", sqlite3.connect("db/event_history.db"))
```
