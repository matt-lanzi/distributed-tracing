# ğŸ§  Distributed Log Normalization & Traceability POC

## ğŸ¯ Project Purpose & Goals
This project demonstrates how to build a robust, real-world distributed tracing and log normalization pipeline using Python, Kafka, and SQLite. It showcases techniques for ingesting, normalizing, and analyzing logs from heterogeneous systemsâ€”structured and unstructuredâ€”enabling full traceability and observability across distributed architectures.

## âœ¨ Features
- Ingests logs from multiple distributed systems (structured JSON and unstructured text)
- Normalizes diverse log formats into a unified schema
- Joins multi-part logs (e.g., checkpoint and error details)
- Stores normalized events in SQLite for easy querying and analytics
- Provides interactive dashboards for event, span, and trace visualization (Streamlit + Plotly)
- Includes a modular flow engine for simulating system behavior and failures
- Kafka UI for inspecting topic messages
- Test coverage for core modules

## ğŸ›  Tech Stack
- Python 3.9+
- Apache Kafka (via Docker Compose)
- SQLite
- Streamlit (dashboards)
- Plotly (visualizations)
- pandas (data analysis)
- pytest (testing)

## ğŸš€ Demo
<!--
Add screenshots or GIFs of your dashboards and/or sample queries here.
Example:
![Events Dashboard Screenshot](docs/events_dashboard.png)
![Trace Dashboard Screenshot](docs/trace_dashboard.png)
-->

---

## âœ… Overview

This system:

- Ingests logs via **Kafka** from multiple distributed systems (structured and unstructured)
- Normalizes log events to a consistent schema
- Joins multi-part logs (e.g., events and error details)
- Stores results in **SQLite** for querying and analysis
- Provides test coverage for key modules
- Includes Kafka UI (optional) for inspecting topic messages

---

## ğŸ— Architecture

```
+-------------------+      +------------------+      +------------------+      +-------------------+
|   inventory         |      |      payments         |      |      orders        |      |    reporting      |
+-------------------+      +------------------+      +------------------+      +-------------------+
        |                          |                        |                          |
        +-----------+--------------+------------------------+--------------------------+
                                    |
                                    v
                         +----------------------+
                         |   Kafka Topic:       |
                         |      raw-logs        |
                         +----------------------+
                                    |
                                    v
                         +----------------------+
                         |   Kafka Consumer     |
                         |   (Normalizer)       |
                         +----------------------+
                                    |
                                    v
                         +----------------------+
                         |   SQLite Database    |
                         |   event_history.db   |
                         +----------------------+
                                    |
                                    v
                         +----------------------+
                         | Export / Analytics   |
                         +----------------------+
```


---

## ğŸ’¾ Normalized Schema

| Field           | Description                                |
|----------------|--------------------------------------------|
| `system_id`     | Originating system (e.g. `inventory`, `orders`) |
| `checkpoint_id` | Processing stage or label                  |
| `timestamp`     | Event timestamp (ISO format)               |
| `status`        | `SUCCESS` or `FAILURE`                     |
| `correlation_id`| Trace ID for joining related events        |
| `failure_reason`| Optional reason for failure (if any)       |

---

## ğŸ”Œ Systems Overview

Each system (producer) represents a different real-world logging style, allowing us to demonstrate normalization of diverse formats.

| System       | Format             | Failure Detail             | Notes                                                                 |
|--------------|--------------------|-----------------------------|-----------------------------------------------------------------------|
| `inventory`    | Structured JSON     | Separate JSON log           | Emits a checkpoint log, and if failed, emits a second log with error detail |
| `reporting`  | Structured JSON     | Inline in log               | Emits one structured log that includes both the event and error fields |
| `orders`       | Unstructured text   | Inline in log               | Emits plain-text logs with both status and failure reason included    |
| `payments`        | Unstructured text   | Separate plain-text log     | Similar to `inventory` but logs are in plain text instead of JSON       |

---

## ğŸ—‚ Supported Systems and Log Examples

### âœ… `inventory` (structured, multi-log join)
- **Main log:** JSON with `status` and `checkpoint`
- **Failure reason:** separate log with same `correlation_id`

**Checkpoint log:**
```
{
  "system": "inventory",
  "checkpoint": "CHECKPOINT_1",
  "status": "FAILURE",
  "timestamp": "2025-05-04T10:30:00",
  "correlation_id": "abc-123"
}
```

**Separate error log:**
```
{
  "system": "inventory",
  "correlation_id": "abc-123",
  "failure_reason": "FAILURE_REASON_2"
}
```
### âœ… `reporting` (structured, inline error)
- Single JSON log with failure reason inline if `status == "FAILURE"`
```
{
  "system": "reporting",
  "checkpoint": "REPORTING_CHECKPOINT_2",
  "status": "FAILURE",
  "timestamp": "2025-05-04T10:33:00",
  "correlation_id": "ghi-789",
  "failure_reason": "REPORTING_FAILURE_REASON_2"
}
```


### âœ… `payments` (unstructured, multi-log join)
**Checkpoint log:**
```
2025-05-04T10:32:00 PAYMENTS: PAYMENTS_CHECKPOINT_2 - FAILURE (ID: def-456)
```

**Separate error log:**
```
2025-05-04T10:32:01 PAYMENTS PAYMENTS_ERROR: (ID: def-456) - PAYMENTS_FAILURE_REASON_1
```

### âœ… `orders` (unstructured, single-log)
**Checkpoint log:**
```
2025-05-04T10:30:00 ORDER: CHECKPOINT_B - FAILURE (ID: abc-123) - FAILURE_REASON_X
```
```
2025-05-04T10:30:00 ORDER: CHECKPOINT_A - SUCCESS (ID: abc-123)
```
---

Example Flow from Orchestrator:
-------------------------------
```
1. Orchestrator starts and triggers a new correlation_id

2. inventory system begins:
   â†’ INITIATED (SUCCESS)
   â†’ CHECKPOINT_1 (FAILURE)
     â†³ [Failure reason emitted separately]
   â†’ CHECKPOINT_1 (SUCCESS)
   â†’ CHECKPOINT_2 (SUCCESS)
   â†’ FINALIZED (SUCCESS)

3. Orchestrator randomly chooses the next downstream path:
   a. inventory â†’ payments â†’ Reporting and ORDER in parallel
   b. inventory â†’ Reporting and ORDER in parallel

4. If PAYMENTS is chosen:
   â†’ PAYMENTS: PAYMENTS_CHECKPOINT_1 (SUCCESS)
   â†’ PAYMENTS: PAYMENTS_CHECKPOINT_2 (FAILURE)
     â†³ PAYMENTS_ERROR log sent separately with failure reason
   â†’ PAYMENTS: PAYMENTS_CHECKPOINT_2 (SUCCESS)
   â†’ PAYMENTS: PAYMENTS_FINALIZED (SUCCESS)

5. Reporting system logs:
   â†’ REPORTING_INITIATED â†’ REPORTING_CHECKPOINT_1 â†’ ... â†’ REPORTING_FINALIZED  


6. ORDER system logs:
   â†’ ORDER_INITIALIZED â†’ ORDER_CHECKPOINT_1 â†’ ... â†’ ORDER_FINALIZED  


7. Each step emits logs to Kafka â†’ parsed â†’ normalized â†’ stored in SQLite
```

---

## ğŸ“‚ Project Structure

```bash
distributed-tracing/
â”œâ”€â”€ README.md
â”œâ”€â”€ orchestrator.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ schema_setup.py
â”‚   â””â”€â”€ event_history.db
â”‚
â”œâ”€â”€ consumer.py
â”‚
â”œâ”€â”€ flow_engine/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ flow_engine.py
â”‚
â”œâ”€â”€ log_parsers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ structured.py
â”‚   â”œâ”€â”€ orders.py
â”‚   â””â”€â”€ payments.py
â”‚
â”œâ”€â”€ producers/
â”‚   â”œâ”€â”€ producer_inventory.py
â”‚   â”œâ”€â”€ producer_payments.py
â”‚   â”œâ”€â”€ producer_orders.py
â”‚   â””â”€â”€ producer_reporting.py
â”‚
â”œâ”€â”€ kui/
â”‚   â””â”€â”€ config.yml
â”‚
# â””â”€â”€ tests/
#     â”œâ”€â”€ conftest.py
#     â”œâ”€â”€ test_flow_engine.py
#     â””â”€â”€ test_parsers/
#         â”œâ”€â”€ test_structured.py
#         â”œâ”€â”€ test_orders.py
#         â””â”€â”€ test_payments.py
```

---

## ğŸš€ Getting Started

### 1. Create Virtual Environment & Install Dependencies

```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Start Kafka + Zookeeper + Kafka UI

```bash
docker-compose up -d
```

### 3. Set Up SQLite Schema

```bash
python db/schema_setup.py
```

### 4. Run the Consumer

```bash
python consumer.py
```

### 5. Run the Orchestrator

```bash
python orchestrator.py
```

---

## ğŸ§ª Run Tests

```bash
PYTHONPATH=. pytest tests/
```

---



---

## ğŸ§° Helpful Commands

### View Kafka Logs Persisted
```bash
sqlite3 db/event_history.db
SELECT * FROM events;
```

### Export to Pandas (in Python)
```python
import pandas as pd
df = pd.read_sql("SELECT * FROM events", sqlite3.connect("db/event_history.db"))
```

---

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
